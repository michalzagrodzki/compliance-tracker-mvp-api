import logging
from typing import Dict, List, Any, Union
from fastapi import HTTPException
from openai import OpenAI
from config.config import settings
from services.schemas import ComplianceGap

logger = logging.getLogger(__name__)

def generate_executive_summary_debug(
    audit_report: Dict[str, Any],
    compliance_gaps: List[Dict[str, Any]],
    summary_type: str = "standard",
) -> str:
    """
    Generate an executive summary using OpenAI API based on audit report and compliance gaps.
    
    Args:
        audit_report: Full audit report object with all metadata
        compliance_gaps: List of compliance gap objects
        summary_type: Type of summary to generate (standard, detailed, brief)
    
    Returns:
        Formatted markdown executive summary
    """
    
    print("=" * 60)
    print("EXECUTIVE SUMMARY GENERATION - DEBUG OUTPUT")
    print("=" * 60)
    
    print(f"Summary Type: {summary_type}")
    print(f"Audit Report Keys: {list(audit_report.keys())}")
    print(f"Number of Compliance Gaps: {len(compliance_gaps)}")
    
    # Build context from audit report
    print("\n--- Building Audit Context ---")
    audit_context = _build_audit_context(audit_report)
    print(f"Audit Context Length: {len(audit_context)} characters")
    print(f"Audit Context Preview (first 200 chars): {audit_context[:200]}...")
    
    # Build compliance gaps analysis
    print("\n--- Building Gaps Analysis ---")
    gaps_analysis = _build_gaps_analysis(compliance_gaps)
    print(f"Gaps Analysis Length: {len(gaps_analysis)} characters")
    print(f"Gaps Analysis Preview (first 200 chars): {gaps_analysis[:200]}...")
    
    # Build summary statistics
    print("\n--- Building Summary Statistics ---")
    summary_stats = _build_summary_statistics(audit_report, compliance_gaps)
    print(f"Summary Stats Length: {len(summary_stats)} characters")
    print(f"Summary Stats Preview (first 200 chars): {summary_stats[:200]}...")
    
    # Create the prompt based on summary type
    print("\n--- Creating Summary Prompt ---")
    prompt = _create_summary_prompt(
        audit_context,
        gaps_analysis,
        summary_stats,
        summary_type,
    )
    
    print(f"Final Prompt Length: {len(prompt)} characters")
    print("\n" + "=" * 60)
    print("FULL PROMPT CONTENT:")
    print("=" * 60)
    print(prompt)
    print("=" * 60)
    
    # System message content
    system_message = (
        "You are an expert compliance analyst and executive report writer. "
        "Generate professional, concise, and actionable executive summaries "
        "for compliance audit reports. Use clear business language suitable "
        "for C-level executives and compliance teams. Format responses in "
        "clean markdown with appropriate headers and bullet points."
    )
    
    print("\n--- System Message ---")
    print(f"System Message Length: {len(system_message)} characters")
    print(f"System Message: {system_message}")
    
    # OpenAI API call would happen here, but we're returning stub instead
    print("\n--- OpenAI API Call (STUBBED) ---")
    print(f"Model: {settings.openai_model}")
    print("Temperature: 0.1")
    print("Max Tokens: 2000")
    
    # Return stub value instead of calling OpenAI
    stub_summary = f"""# Executive Summary - {audit_report.get('report_title', 'Compliance Audit Report')}

## Overview
This is a **STUB RESPONSE** for testing purposes. The actual executive summary would be generated by OpenAI API.

## Key Findings
- **Total Compliance Gaps**: {len(compliance_gaps)}
- **Summary Type**: {summary_type}

## Risk Assessment
- High Risk Issues: [STUB DATA]
- Medium Risk Issues: [STUB DATA]  
- Low Risk Issues: [STUB DATA]

## Recommendations
1. Review the debug output above to verify prompt generation
2. Check that all helper functions are working correctly
3. Validate the prompt structure and content

## Next Steps
- Implement actual OpenAI API call when ready
- Test with real audit data
- Validate output formatting

---
*This summary was generated for debugging purposes only.*
"""
    
    print("\n--- Generated Stub Summary ---")
    print(f"Stub Summary Length: {len(stub_summary)} characters")
    print("Stub Summary Content:")
    print(stub_summary)
    
    logger.info(f"DEBUG: Generated stub executive summary for audit report "
                f"'{audit_report.get('report_title', 'Unknown')}' with {len(compliance_gaps)} gaps")
    
    print("=" * 60)
    print("END OF DEBUG OUTPUT")
    print("=" * 60)
    
    return stub_summary

def generate_executive_summary(
    audit_report: Dict[str, Any],
    compliance_gaps: List[ComplianceGap],
    summary_type: str = "standard",
) -> str:
    """
    Generate an executive summary using OpenAI API based on audit report and compliance gaps.
    
    Args:
        audit_report: Full audit report object with all metadata
        compliance_gaps: List of compliance gap objects
        summary_type: Type of summary to generate (standard, detailed, brief)
    
    Returns:
        Formatted markdown executive summary
    """
    
    # Build context from audit report
    audit_context = _build_audit_context(audit_report)
    
    # Build compliance gaps analysis
    gaps_analysis = _build_gaps_analysis(compliance_gaps)
    
    # Build summary statistics
    summary_stats = _build_summary_statistics(audit_report, compliance_gaps)
    
    # Create the prompt based on summary type
    system_message, user_prompt = _create_summary_prompt(
        audit_context, 
        gaps_analysis, 
        summary_stats, 
        summary_type,
    )
    
    client = OpenAI(api_key=settings.openai_api_key)
    
    try:
        completion = client.chat.completions.create(
            model=settings.openai_model,
            messages=[
                {
                    "role": "system",
                    "content": system_message
                },
                {
                    "role": "user",
                    "content": user_prompt
                }
            ],
            temperature=0.1,
            max_tokens=2000,
        )
    except Exception as e:
        logger.error("OpenAI ChatCompletion failed for executive summary", exc_info=True)
        raise HTTPException(status_code=502, detail=f"OpenAI API error: {e}")
    
    summary = completion.choices[0].message.content.strip()
    
    logger.info(f"Successfully generated executive summary for audit report "
               f"'{audit_report.get('report_title', 'Unknown')}' with {len(compliance_gaps)} gaps")
    
    return summary

def _get_gap_value(gap: Union[Dict[str, Any], ComplianceGap], key: str, default: Any = None) -> Any:
    """Helper function to get value from gap object regardless of type."""
    if isinstance(gap, dict):
        return gap.get(key, default)
    else:
        return getattr(gap, key, default)

def _build_audit_context(audit_report: Dict[str, Any]) -> str:
    """Build audit context section from audit report data."""
    
    context_parts = []
    
    # Basic audit information
    context_parts.append(f"**Audit Title:** {audit_report.get('report_title', 'N/A')}")
    context_parts.append(f"**Compliance Domain:** {audit_report.get('compliance_domain', 'N/A')}")
    context_parts.append(f"**Report Type:** {audit_report.get('report_type', 'N/A')}")
    context_parts.append(f"**Target Audience:** {audit_report.get('target_audience', 'N/A')}")
    context_parts.append(f"**Confidentiality Level:** {audit_report.get('confidentiality_level', 'N/A')}")
    
    # Audit scope metrics
    context_parts.append(f"**Documents Reviewed:** {len(audit_report.get('document_ids', []))}")
    context_parts.append(f"**Chat Sessions:** {len(audit_report.get('chat_history_ids', []))}")
    context_parts.append(f"**PDF Sources:** {len(audit_report.get('pdf_ingestion_ids', []))}")
    
    # Configuration details
    if audit_report.get('include_technical_details', False):
        context_parts.append("**Technical Details:** Included")
    if audit_report.get('include_source_citations', False):
        context_parts.append("**Source Citations:** Included")
    if audit_report.get('external_auditor_access', False):
        context_parts.append("**External Auditor Access:** Granted")
    
    return "\n".join(context_parts)

def _analyze_recommendations(compliance_gaps: List[Union[Dict[str, Any], ComplianceGap]]) -> str:
    """Analyze recommendation-related properties of compliance gaps."""
    
    recommendation_parts = []
    
    # Count gaps with recommendations
    gaps_with_recommendations = 0
    gaps_with_actions = 0
    recommendation_types = {}
    total_actions = 0
    
    for gap in compliance_gaps:
        # Check for recommendation text
        rec_text = _get_gap_value(gap, 'recommendation_text', None)
        if rec_text and rec_text.strip():
            gaps_with_recommendations += 1
        
        # Check for recommendation type
        rec_type = _get_gap_value(gap, 'recommendation_type', None)
        if rec_type:
            recommendation_types[rec_type] = recommendation_types.get(rec_type, 0) + 1
        
        # Check for recommended actions
        rec_actions = _get_gap_value(gap, 'recommended_actions', None)
        if rec_actions and isinstance(rec_actions, list) and len(rec_actions) > 0:
            gaps_with_actions += 1
            total_actions += len(rec_actions)
    
    # Build recommendation summary
    if gaps_with_recommendations > 0 or gaps_with_actions > 0:
        recommendation_parts.append("**Recommendations Summary:**")
        
        if gaps_with_recommendations > 0:
            recommendation_parts.append(f"- {gaps_with_recommendations} gaps have detailed recommendations")
        
        if gaps_with_actions > 0:
            recommendation_parts.append(f"- {gaps_with_actions} gaps have actionable steps ({total_actions} total actions)")
        
        # Recommendation types breakdown
        if recommendation_types:
            recommendation_parts.append("- **Recommendation Types:**")
            for rec_type, count in recommendation_types.items():
                recommendation_parts.append(f"  - {rec_type.replace('_', ' ').title()}: {count} gaps")
    
    return "\n".join(recommendation_parts)

def _analyze_related_documents(compliance_gaps: List[Union[Dict[str, Any], ComplianceGap]]) -> str:
    """Analyze related documents properties of compliance gaps."""
    
    documents_parts = []
    
    # Count gaps with related documents
    gaps_with_documents = 0
    total_documents = 0
    unique_documents = set()
    
    for gap in compliance_gaps:
        related_docs = _get_gap_value(gap, 'related_documents', None)
        if related_docs and isinstance(related_docs, list) and len(related_docs) > 0:
            gaps_with_documents += 1
            total_documents += len(related_docs)
            unique_documents.update(related_docs)
    
    # Build documents summary
    if gaps_with_documents > 0:
        documents_parts.append("**Related Documents Analysis:**")
        documents_parts.append(f"- {gaps_with_documents} gaps reference related documents")
        documents_parts.append(f"- {total_documents} total document references")
        documents_parts.append(f"- {len(unique_documents)} unique documents referenced")
        
        # Show most frequently referenced documents if available
        if len(unique_documents) > 0:
            # Count document frequency
            doc_frequency = {}
            for gap in compliance_gaps:
                related_docs = _get_gap_value(gap, 'related_documents', None)
                if related_docs and isinstance(related_docs, list):
                    for doc in related_docs:
                        doc_frequency[doc] = doc_frequency.get(doc, 0) + 1
            
            # Show top 3 most referenced documents
            if doc_frequency:
                sorted_docs = sorted(doc_frequency.items(), key=lambda x: x[1], reverse=True)
                top_docs = sorted_docs[:3]
                
                documents_parts.append("- **Most Referenced Documents:**")
                for doc, count in top_docs:
                    # Truncate long document names
                    display_name = doc if len(doc) <= 50 else f"{doc[:47]}..."
                    documents_parts.append(f"  - {display_name} ({count} gaps)")
    
    return "\n".join(documents_parts)

def _build_gaps_analysis(compliance_gaps: List[Union[Dict[str, Any], ComplianceGap]]) -> str:
    """Build compliance gaps analysis section with comprehensive breakdown."""
    
    if not compliance_gaps:
        return "**No compliance gaps identified in this audit.**"
    
    analysis_parts = []
    
    # Group gaps by risk level
    risk_groups = {}
    for gap in compliance_gaps:
        risk_level = _get_gap_value(gap, 'risk_level', 'unknown')
        if risk_level not in risk_groups:
            risk_groups[risk_level] = []
        risk_groups[risk_level].append(gap)

    # Group gaps by category
    category_groups = {}
    for gap in compliance_gaps:
        category = _get_gap_value(gap, 'gap_category', 'uncategorized')
        if category not in category_groups:
            category_groups[category] = []
        category_groups[category].append(gap)

    # Group gaps by business impact
    business_impact_groups = {}
    for gap in compliance_gaps:
        impact = _get_gap_value(gap, 'business_impact', 'unknown')
        if impact not in business_impact_groups:
            business_impact_groups[impact] = []
        business_impact_groups[impact].append(gap)

    # Risk level breakdown
    analysis_parts.append("**Risk Level Distribution:**")
    for risk_level in ['high', 'medium', 'low']:
        count = len(risk_groups.get(risk_level, []))
        if count > 0:
            analysis_parts.append(f"- {risk_level.title()}: {count} gaps")

    # Business impact breakdown
    analysis_parts.append("\n**Business Impact Distribution:**")
    for impact_level in ['high', 'medium', 'low']:
        count = len(business_impact_groups.get(impact_level, []))
        if count > 0:
            analysis_parts.append(f"- {impact_level.title()}: {count} gaps")

    # Category breakdown
    analysis_parts.append("\n**Gap Categories:**")
    for category, gaps in category_groups.items():
        analysis_parts.append(f"- {category}: {len(gaps)} gaps")

    # Detailed gaps breakdown by risk level
    analysis_parts.append("\n**Detailed Gap Analysis:**")
    
    # Show high-risk gaps first
    for risk_level in ['high', 'medium', 'low']:
        gaps_in_level = risk_groups.get(risk_level, [])
        if gaps_in_level:
            analysis_parts.append(f"\n**{risk_level.title()} Risk Gaps ({len(gaps_in_level)}):**")
            
            for i, gap in enumerate(gaps_in_level, 1):
                gap_title = _get_gap_value(gap, 'gap_title', f'Gap #{i}')
                gap_description = _get_gap_value(gap, 'gap_description', 'No description available')
                original_question = _get_gap_value(gap, 'original_question', 'No original question recorded')
                gap_category = _get_gap_value(gap, 'gap_category', 'uncategorized')
                business_impact = _get_gap_value(gap, 'business_impact', 'unknown')
                
                # Truncate long descriptions for readability
                truncated_description = gap_description[:200] + "..." if len(gap_description) > 200 else gap_description
                truncated_question = original_question[:150] + "..." if len(original_question) > 150 else original_question
                
                analysis_parts.append(f"\n{i}. **{gap_title}**")
                analysis_parts.append(f"   - *Description:* {truncated_description}")
                analysis_parts.append(f"   - *Original Question:* {truncated_question}")
                analysis_parts.append(f"   - *Category:* {gap_category}")
                analysis_parts.append(f"   - *Business Impact:* {business_impact}")
                
                # Add additional relevant details if available
                detection_method = _get_gap_value(gap, 'detection_method', None)
                if detection_method and detection_method != 'unknown':
                    analysis_parts.append(f"   - *Detection Method:* {detection_method.replace('_', ' ').title()}")
                
                regulatory_req = _get_gap_value(gap, 'regulatory_requirement', False)
                if regulatory_req:
                    analysis_parts.append(f"   - *Regulatory Requirement:* Yes")
                
                potential_fine = _get_gap_value(gap, 'potential_fine_amount', 0)
                if potential_fine and potential_fine > 0:
                    if hasattr(potential_fine, '__float__'):
                        potential_fine = float(potential_fine)
                    analysis_parts.append(f"   - *Potential Fine:* ${potential_fine:,.2f}")

    # Regulatory requirements
    regulatory_gaps = [gap for gap in compliance_gaps if _get_gap_value(gap, 'regulatory_requirement', False)]
    if regulatory_gaps:
        analysis_parts.append(f"\n**Regulatory Requirements:** {len(regulatory_gaps)} gaps require regulatory compliance")

    # Financial impact analysis
    total_potential_fines = 0
    gaps_with_fines = 0
    for gap in compliance_gaps:
        fine_amount = _get_gap_value(gap, 'potential_fine_amount', 0)
        if fine_amount:
            # Handle Decimal objects
            if hasattr(fine_amount, '__float__'):
                fine_amount = float(fine_amount)
            total_potential_fines += fine_amount
            gaps_with_fines += 1
    
    if total_potential_fines > 0:
        analysis_parts.append(f"\n**Potential Financial Impact:** ${total_potential_fines:,.2f} across {gaps_with_fines} gaps")

    # Questions analysis - analyze patterns in original questions
    analysis_parts.append("\n**Question Pattern Analysis:**")
    question_keywords = {}
    total_questions = 0
    
    for gap in compliance_gaps:
        original_question = _get_gap_value(gap, 'original_question', '')
        if original_question and original_question.strip():
            total_questions += 1
            # Extract common keywords from questions (simple word frequency analysis)
            words = original_question.lower().split()
            for word in words:
                # Filter out common stop words and focus on meaningful terms
                if len(word) > 3 and word not in ['what', 'where', 'when', 'how', 'why', 'does', 'this', 'that', 'with', 'from', 'they', 'have', 'been', 'were', 'will', 'would', 'could', 'should']:
                    question_keywords[word] = question_keywords.get(word, 0) + 1
    
    if question_keywords:
        # Show top 5 most frequent meaningful keywords
        top_keywords = sorted(question_keywords.items(), key=lambda x: x[1], reverse=True)[:5]
        analysis_parts.append(f"- {total_questions} questions analyzed")
        analysis_parts.append("- Most common question topics:")
        for keyword, count in top_keywords:
            analysis_parts.append(f"  - '{keyword}': {count} occurrences")

    # Recommendation analysis
    recommendation_analysis = _analyze_recommendations(compliance_gaps)
    if recommendation_analysis:
        analysis_parts.append(f"\n{recommendation_analysis}")

    # Related documents analysis
    documents_analysis = _analyze_related_documents(compliance_gaps)
    if documents_analysis:
        analysis_parts.append(f"\n{documents_analysis}")

    # Detection method breakdown
    detection_methods = {}
    for gap in compliance_gaps:
        method = _get_gap_value(gap, 'detection_method', 'unknown')
        detection_methods[method] = detection_methods.get(method, 0) + 1

    if detection_methods:
        analysis_parts.append("\n**Detection Methods:**")
        for method, count in detection_methods.items():
            analysis_parts.append(f"- {method.replace('_', ' ').title()}: {count} gaps")
    
    return "\n".join(analysis_parts)

def _build_summary_statistics(audit_report: Dict[str, Any], compliance_gaps: List[Union[Dict[str, Any], ComplianceGap]]) -> str:
    stats_parts = []

    total_gaps = len(compliance_gaps)
    stats_parts.append(f"**Total Compliance Gaps:** {total_gaps}")

    if compliance_gaps:
        confidence_scores = []
        false_positive_scores = []
        
        for gap in compliance_gaps:
            confidence = _get_gap_value(gap, 'confidence_score', 0)
            false_positive = _get_gap_value(gap, 'false_positive_likelihood', 0)

            if hasattr(confidence, '__float__'):
                confidence = float(confidence)
            if hasattr(false_positive, '__float__'):
                false_positive = float(false_positive)
                
            confidence_scores.append(confidence)
            false_positive_scores.append(false_positive)
        
        if confidence_scores:
            avg_confidence = sum(confidence_scores) / len(confidence_scores)
            stats_parts.append(f"**Average Confidence Score:** {avg_confidence:.2f}")
        
        if false_positive_scores:
            avg_false_positive = sum(false_positive_scores) / len(false_positive_scores)
            stats_parts.append(f"**Average False Positive Likelihood:** {avg_false_positive:.2f}")
    
    detection_methods = {}
    for gap in compliance_gaps:
        method = _get_gap_value(gap, 'detection_method', 'unknown')
        detection_methods[method] = detection_methods.get(method, 0) + 1
    
    if detection_methods:
        stats_parts.append("**Detection Methods:**")
        for method, count in detection_methods.items():
            stats_parts.append(f"- {method}: {count} gaps")
    
    return "\n".join(stats_parts)

def _create_summary_prompt(
    audit_context: str,
    gaps_analysis: str,
    summary_stats: str,
    summary_type: str,
) -> tuple[str, str]:
    
    # System message to set the tone and context
    system_message = (
        "You are an expert compliance analyst and executive report writer. "
        "Generate professional, concise, and actionable executive summaries "
        "for compliance audit reports. Use clear business language suitable "
        "for C-level executives and compliance teams. Format responses in "
        "clean markdown with appropriate headers and bullet points."
    )
    
    user_prompt = f"""
Please generate a professional executive summary for the following compliance audit report.

## Audit Context
{audit_context}

## Compliance Gaps Analysis
{gaps_analysis}

## Summary Statistics
{summary_stats}

## Summary Requirements
- Format: Professional markdown suitable for executive presentation
- Tone: Clear, concise, business-focused
- Include: Key findings, risk assessment, actionable recommendations
- Structure: Executive overview, key findings, risk prioritization, next steps
"""

    # Add summary type specific instructions
    type_instructions = {
        "standard": "Create a comprehensive executive summary (800-1200 words) covering all key aspects.",
        "brief": "Create a concise executive summary (300-500 words) focusing on critical findings only.",
        "detailed": "Create a detailed executive summary (1200-2000 words) with in-depth analysis and recommendations."
    }
    
    user_prompt += f"\n- Length: {type_instructions.get(summary_type, type_instructions['standard'])}"
    user_prompt += """

Generate the executive summary now, using clear markdown formatting with appropriate headers and bullet points.
"""
    
    return system_message, user_prompt